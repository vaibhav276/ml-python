{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression: Binary classification\n",
    "\n",
    "In this notebook, we are going to perform binary classification (two classes) using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Radar data\n",
    "\n",
    "This example uses data generated by radars after scanning ionosphere for some time to detect presence of free electrons in ionosphere. Original source of data: https://archive.ics.uci.edu/ml/datasets/Ionosphere\n",
    "\n",
    "The dataset contains 35 columns - first 34 columns (labelled 0 to 33) are readings of various parameters. The 35th column ( labelled 34) is either 'g' or 'b' indicating 'good' or 'bad', where 'good' means presence of free electrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as op\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1        2        3        4        5        6        7        8   \\\n",
       "0   1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
       "1   1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
       "2   1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
       "3   1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
       "4   1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
       "\n",
       "        9  ...       25       26       27       28       29       30       31  \\\n",
       "0  0.03760 ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267 -0.54487   \n",
       "1 -0.04549 ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626 -0.06288   \n",
       "2  0.01198 ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436 -0.24180   \n",
       "3  0.00000 ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682  1.00000   \n",
       "4 -0.16399 ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707 -0.59573   \n",
       "\n",
       "        32       33  34  \n",
       "0  0.18641 -0.45300   g  \n",
       "1 -0.13738 -0.02447   b  \n",
       "2  0.56045 -0.38238   g  \n",
       "3 -0.32382  1.00000   b  \n",
       "4 -0.04608 -0.65697   g  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('bin_classifier_ionosphere.csv', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 35)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How logistic regression works?\n",
    "\n",
    "In logistic regression (binary classification), we map input feature variables to a discrete output value (either 0 or 1). For doing that, we use [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), which is bound between values 0 and 1.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/SigmoidFunction.png/400px-SigmoidFunction.png\">\n",
    "\n",
    "Formally, sigmoid function is:\n",
    "\n",
    "$$\n",
    "h(z) = \\frac {1} {1-e^{-z}}\n",
    "$$\n",
    "\n",
    "So, we map input features to output features by first calculating\n",
    "\n",
    "$$\n",
    "z = \\theta^T X\n",
    "$$\n",
    "\n",
    "and then applying sigmoid function to $z$\n",
    "\n",
    "The sigmoid is a continuous function, so we obtain discrete output from it as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h(z) \\ge 0.5 \\rightarrow y = 1 \\\\\n",
    "h(z) \\lt 0.5 \\rightarrow y = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The following function implements a hypothesis for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Hypothesis function\n",
    "def hypothesisLogisticRegression(theta, X):\n",
    "    sig = np.vectorize(sigmoid)\n",
    "    G = np.dot(X, theta)\n",
    "    theta = theta.reshape(X.shape[1], 1) # optimizer rehsapes it to row vector, so we reshape it back to column vector\n",
    "    return sig(np.dot(X, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the hypothesis - The cost function\n",
    "\n",
    "Any value of $\\theta$ represents some decision boundary. The cost function should evaluate how good a given value of $\\theta$ is by using that on training examples and calculating how far the hypothesis is from the ground truth values. For a given training example, when we apply our hypothesis function, we are going to get continous value between 0 and 1. If the ground truth is 0 for that example, we should penalize more if the hypothesis is close to 1, and if the ground truth is 1, we should penalize the hypothesis if it is closer to 0. Moreover the penalty should grow exponentially with distance from ground truth.\n",
    "\n",
    "Therefore, in order to get an exponentially growing penalty, we use the $log$ function, which has the following property:\n",
    "\n",
    "$-log(x)$ is an exponentially decreasing function, which intercepts x axis at x = 1.\n",
    "\n",
    "<img src = \"http://www.rapidtables.com/math/algebra/logarithm/log-graph.png\" />\n",
    "\n",
    "So if we say $cost = -log(x)$, then for ground truth value 1, we get cost = 0 for x = 1 and cost = $+\\infty$ for x = 0.\n",
    "\n",
    "Similarly, we cau use the same function for ground truth value 0 by saying $cost = -log(1-x)$.\n",
    "\n",
    "Therefore our cost function looks like:\n",
    "\n",
    "$$\n",
    "J = \n",
    "\\begin{cases}\n",
    "-log(h_\\theta(x)),  & \\text{if y = 1} \\\\\n",
    "-log(1 - h_\\theta(x)), & \\text{if y = 0}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We can even combine the two conditions and have a simplified version of the cost function as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J & = - ylog(h_\\theta(x)) - (1-y)log(1 - h_\\theta(x)) \\\\\n",
    "& = - [ylog(h_\\theta(x)) + (1-y)log(1 - h_\\theta(x))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For all training examples, the cost function would become:\n",
    "\n",
    "$$\n",
    "J = -\\frac1m \\sum_{i=1}^m {y^{(i)}.log(h_\\theta(x^{(i)}) + (1-y^{(i)})log(1 - h_\\theta(x^{(i)}) }\n",
    "$$\n",
    "\n",
    "The following code implements this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic regression cost function\n",
    "def costLogisticRegression(theta, X, y):\n",
    "    H = hypothesisLogisticRegression(theta, X)\n",
    "    m, n = X.shape\n",
    "    theta = theta.reshape(n, 1) # optimizer rehsapes it to row vector, so we reshape it back to column vector\n",
    "    J = (-1/m) * ( np.dot(y.T, np.log(H)) + np.dot((1-y).T, np.log(1 - H)) )\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal values of coefficients $\\theta$\n",
    "\n",
    "Just like [Linear regression](\"../linear_regression/1_multiple_variables.ipynb\"), we can apply gradient descent to logistic regression also.\n",
    "\n",
    "Gradient descent in this case is given by:\n",
    "\n",
    "repeat till convergence: {\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_0^{(i)} \\\\\n",
    "\\theta_1 := \\theta_0 - \\alpha \\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_1^{(i)} \\\\\n",
    "\\theta_2 := \\theta_0 - \\alpha \\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_2^{(i)} \\\\\n",
    "... \\\\\n",
    "\\theta_n := \\theta_0 - \\alpha \\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_n^{(i)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "}\n",
    "\n",
    "Instead of manually iterating and reaching to min value of cost function, we can also use built in functions provided by scipy library to get the optimal values of $\\theta$. We are going to do that in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient function to be used by optimizer\n",
    "def gradLogisticRegression(theta, X, y):\n",
    "    H = hypothesisLogisticRegression(theta, X)\n",
    "    m, n = X.shape\n",
    "    grad = (1/m) * (np.dot( (H - y).T , X ).T)\n",
    "    return grad\n",
    "\n",
    "# Gradient descent - optimizes cost\n",
    "def gradientDescentLogisticRegression(theta_init, X, y, maxiter):\n",
    "    res = op.minimize(fun = costLogisticRegression, \n",
    "                                 x0 = theta_init, \n",
    "                                 args = (X, y),\n",
    "                                 method = 'TNC',\n",
    "                                 jac = gradLogisticRegression,\n",
    "                                 options = {\n",
    "                                    'maxiter': maxiter,\n",
    "                                    'disp': True #  No idea why its not working\n",
    "                                    }\n",
    "                                 );\n",
    "    return res.x # optimal theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying algorithm on data\n",
    "\n",
    "### Step 1: Extract data into matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract into separate matrices for input and output\n",
    "data = np.matrix(data)\n",
    "\n",
    "# Important to shuffle data before beginning, as input data may be biased towards\n",
    "# some particular classes in the beginning - depending on the method used to obtain examples\n",
    "np.random.shuffle(data) \n",
    "\n",
    "X = data[:,0:34]\n",
    "y = data[:,34]\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "\n",
    "y = (y == 'g') # 'g' -> 1, other values -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Partitioning and scaling\n",
    "\n",
    "We will partition the sample data into two parts - training set and test set. And then use scikit's scaling function to bring all data to same mean and comparable variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_size = X.shape[0]\n",
    "training_set_size = int(total_size * (0.8))\n",
    "test_set_size = total_size - training_set_size\n",
    "\n",
    "Xtrain = X[0:training_set_size, :]\n",
    "XtrainNorm = preprocessing.scale(Xtrain.astype(np.float64)) # scale to mean = 0 and unit variance\n",
    "XtrainNorm = np.append(np.ones((XtrainNorm.shape[0],1)), XtrainNorm, axis = 1)\n",
    "ytrain = y[0:training_set_size]\n",
    "\n",
    "Xtest = X[(training_set_size):(training_set_size + test_set_size), :]\n",
    "XtestNorm = preprocessing.scale(Xtest.astype(np.float64)) # scale to mean = 0 and unit variance\n",
    "XtestNorm = np.append(np.ones((XtestNorm.shape[0],1)), XtestNorm, axis = 1)\n",
    "ytest = y[(training_set_size): (training_set_size + test_set_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Applying gradient descent to minimize cost\n",
    "\n",
    "We'll call the functions defined above for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error (280 examples) [[ 0.15395152]]\n",
      "Test error (71 examples) [[ 0.56039339]]\n"
     ]
    }
   ],
   "source": [
    "theta_init = np.zeros((XtrainNorm.shape[1], 1)) # start from zeros\n",
    "\n",
    "maxiter = 100 # Obtained from trial and error\n",
    "theta = gradientDescentLogisticRegression(theta_init, XtrainNorm, ytrain, maxiter)\n",
    "\n",
    "J_training = costLogisticRegression(theta, XtrainNorm, ytrain)\n",
    "J_test = costLogisticRegression(theta, XtestNorm, ytest)\n",
    "\n",
    "print(\"Training error (\" + str(training_set_size) + \" examples)\", J_training)\n",
    "print(\"Test error (\" + str(test_set_size) + \" examples)\", J_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 4: Calculating accuracy\n",
    "\n",
    "In case of classficiation problems, the end goal is to be able to predict class = 0 or class = 1 for any given input. In our case, when we apply hypothesis function, we are getting a continuous value. So we need to convert the hypothesis into prediction by taking any value greater than a _threshold_ to be a positive hypothesis.\n",
    "\n",
    "Following code performs that and calculated accuracy in percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateAccuracy(theta, X, y, threshold):\n",
    "    m, n = X.shape\n",
    "    H = hypothesisLogisticRegression(theta, X)\n",
    "    \n",
    "    # Predicting anything greater than 'threshold' as positive\n",
    "    P = np.zeros((m, 1))\n",
    "    P[H > threshold] = 1\n",
    "    P[H <= threshold] = 0\n",
    "    \n",
    "    match = np.zeros((m, 1))\n",
    "    match[P == y] = 1\n",
    "    \n",
    "    return (sum(match) * 100 / m) # percentage accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can calculate accuracy of our model for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy (percentage) [ 93.57142857]\n",
      "Test accuracy (percentage) [ 91.54929577]\n"
     ]
    }
   ],
   "source": [
    "# For simple logistic regression, threshold = 0.5 (see How logistic regression works?\" above)\n",
    "a_training = calculateAccuracy(theta, XtrainNorm, ytrain, 0.5)\n",
    "a_test = calculateAccuracy(theta, XtestNorm, ytest, 0.5)\n",
    "\n",
    "print(\"Training accuracy (percentage)\", a_training)\n",
    "print(\"Test accuracy (percentage)\", a_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
