{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression: Multiclass classification and Regularization\n",
    "\n",
    "In this notebook, we cover an example of applying Logistic regression to classify data into more than two classes. We also apply regularization to logistic regression cost function and gradient function.\n",
    "\n",
    "<p>For example of binary classification, refer <a href=\"1_binary_classification.ipynb\">Logistic regression: Binary classification</a>\n",
    "<p>For example of regularization on linear refression, refer: <a href=\"../1_linear_regression/2_multiple_variables_reg.ipynb\">Linear regression: Multiple variables with Regularization</a>\n",
    "\n",
    "## Dataset: Alphabets\n",
    "\n",
    "Original source: https://archive.ics.uci.edu/ml/datasets/Letter+Recognition\n",
    "\n",
    "In this dataset, we have 16 features which can be used to identify one of the 26 possible english alpabets. The 16 features are generated by using different fonts, adding noise and then taking 16 unique components of the result. Each of the 16 features are scaled to values 0-15.\n",
    "\n",
    "Attribute information:\n",
    "1.\tlettr\tcapital letter\t(26 values from A to Z) \n",
    "2.\tx-box\thorizontal position of box\t(integer) \n",
    "3.\ty-box\tvertical position of box\t(integer) \n",
    "4.\twidth\twidth of box\t(integer) \n",
    "5.\thigh height of box\t(integer) \n",
    "6.\tonpix\ttotal # on pixels\t(integer) \n",
    "7.\tx-bar\tmean x of on pixels in box\t(integer) \n",
    "8.\ty-bar\tmean y of on pixels in box\t(integer) \n",
    "9.\tx2bar\tmean x variance\t(integer) \n",
    "10.\ty2bar\tmean y variance\t(integer) \n",
    "11.\txybar\tmean x y correlation\t(integer) \n",
    "12.\tx2ybr\tmean of x * x * y\t(integer) \n",
    "13.\txy2br\tmean of x * y * y\t(integer) \n",
    "14.\tx-ege\tmean edge count left to right\t(integer) \n",
    "15.\txegvy\tcorrelation of x-ege with y\t(integer) \n",
    "16.\ty-ege\tmean edge count bottom to top\t(integer) \n",
    "17.\tyegvx\tcorrelation of y-ege with x\t(integer)\n",
    "\n",
    "The first attribute is the class, which we need to build the model for. The rest of the attributed are the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as op\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0  T   2   8   3   5   1   8  13   0   6   6  10   8   0   8   0   8\n",
       "1  I   5  12   3   7   2  10   5   5   4  13   3   9   2   8   4  10\n",
       "2  D   4  11   6   8   6  10   6   2   6  10   3   7   3   7   3   9\n",
       "3  N   7  11   6   6   3   5   9   4   6   4   4  10   6  10   2   8\n",
       "4  G   2   1   3   1   1   8   6   6   6   6   5   9   1   7   5  10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('letter-recognition.data', header=None)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-all Classification\n",
    "The one-vs-all classification is basically an extension to binary classification. Where binary classification can help us classify whether a given example belongs to class 0 or class 1, we can use $K$ binary classifiers to identify if a given example belongs to class $k$ or not. However, only one of the $K$ classifiers should result in positive output in order to unambiguously classify an example to one of the classes.\n",
    "\n",
    "In other words, we can build $K$ classifiers for $K$ classes, where each classifier would tell us whether an example is positive or negative for that particular classifier. By applying all $K$ classifiers on the example, we can put the example in one of the $K$ classes which tests out positive.\n",
    "\n",
    "In this example, we will build 26 classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and gradient descent\n",
    "\n",
    "We could use the same cost function and gradient descent algorithms as binary classification. However, we are going to add a regularization term to them.\n",
    "\n",
    "The new cost function with regularization term is:\n",
    "\n",
    "$$\n",
    "J = -\\frac1m \\sum_{i=1}^m {y^{(i)}.log(h_\\theta(x^{(i)}) + (1-y^{(i)})log(1 - h_\\theta(x^{(i)}) } + \\frac \\lambda {2m} \\sum_{j = 1}^n \\theta_j^2\n",
    "$$\n",
    "\n",
    "And our new gradient function with regularization is:\n",
    "\n",
    "$$\n",
    "grad_j = \\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_1^{(i)} + \\frac \\lambda m \\theta_j\n",
    "$$\n",
    "\n",
    "where j = 1,2,...n (not 0,1,..n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Hypothesis function\n",
    "def hypothesisLogisticRegression(theta, X):\n",
    "    sig = np.vectorize(sigmoid)\n",
    "    theta = theta.reshape(X.shape[1], 1) # optimizer rehsapes it to row vector, so we reshape it back to column vector\n",
    "    return sig(np.dot(X, theta))\n",
    "\n",
    "# Logistic regression cost function\n",
    "def costLogisticRegression(theta, X, y, lmbda):\n",
    "    H = hypothesisLogisticRegression(theta, X)\n",
    "    m, n = X.shape\n",
    "    theta = theta.reshape(n, 1) # optimizer rehsapes it to row vector, so we reshape it back to column vector\n",
    "    J = (-1/m) * ( np.dot(y.T, np.log(H)) + np.dot((1-y).T, np.log(1 - H)) )\n",
    "    \n",
    "    theta_ = np.copy(theta)\n",
    "    theta_[0][0] = 0\n",
    "    reg = (lmbda / (2*m)) * np.dot(theta_.T, theta_)\n",
    "    J = J + reg\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Gradient function to be used by optimizer\n",
    "def gradLogisticRegression(theta, X, y, lmbda):\n",
    "    H = hypothesisLogisticRegression(theta, X)\n",
    "    m, n = X.shape\n",
    "    grad = (1/m) * (np.dot( (H - y).T , X ).T)\n",
    "    \n",
    "    theta = theta.reshape(n, 1) # optimizer rehsapes it to row vector, so we reshape it back to column vector\n",
    "    theta_ = np.copy(theta)\n",
    "    theta_[0][0] = 0\n",
    "    reg = (lmbda / m) * theta_\n",
    "    grad = grad + reg\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# Gradient descent - optimizes cost\n",
    "def gradientDescentLogisticRegression(theta_init, X, y, maxiter, lmbda):\n",
    "    res = op.minimize(fun = costLogisticRegression, \n",
    "                                 x0 = theta_init, \n",
    "                                 args = (X, y, lmbda),\n",
    "                                 method = 'TNC',\n",
    "                                 jac = gradLogisticRegression,\n",
    "                                 options = {\n",
    "                                    'maxiter': maxiter,\n",
    "                                    'disp': True #  No idea why its not working\n",
    "                                    }\n",
    "                                 );\n",
    "    return res.x # optimal theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the convergence of cost when using gradient descent, we are going to create a wrapper function for gradient descent that will divide the iterations into, say 100 parts and perform gradient descent for each of the part while calculating the intermediate costs after each one. For example, if we are runnnig gradient descent for total of 500 iterations, the wrapper function will run first 5 iterations, calculate cost, then run next 5 iterations, calculate cost, and so on.\n",
    "\n",
    "This way we can plot a graph for intermediate costs and visualize that the cost is actually converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient descent wrapper function. Partitions the input number of iterations\n",
    "# and calculates intermediate costs for each partition\n",
    "# Note: 'maxiter' should be an exact multiple of 'partitions'\n",
    "def gradientDescentWrapper(theta_init, X, y, maxiter, lmbda, partitions):\n",
    "    partition_size = int(maxiter / partitions)\n",
    "    theta_init_next = np.copy(theta_init)\n",
    "    cost_history = np.zeros(partitions)\n",
    "    \n",
    "    for i in range(partitions):\n",
    "        theta_i = gradientDescentLogisticRegression(theta_init_next, X, y, partition_size, lmbda)\n",
    "        cost_history[i] = costLogisticRegression(theta_i, X, y, lmbda)\n",
    "        theta_init_next = np.copy(theta_i)\n",
    "    \n",
    "    return (cost_history, theta_init_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying algorithm on data\n",
    "\n",
    "Now we can apply one-vs-all training algorithm on our data by looping through $K$ labels (26 in this example) and preparing a $\\Theta$ which contains one column for each $\\theta$, which represents one label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : [A...B...C...D...E...F...G...H...I...J...K...L...M...N...O...P...Q...R...S...T...U...V...W...X...Y...Z...]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb67f513588>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOAe5mVnBOcjNzArOQW5mVnAOcjOzgvv/9FGkmDHwk8UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb67f91c2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract into separate matrices for input and output\n",
    "data = np.matrix(data)\n",
    "\n",
    "# Important to shuffle data before beginning, as input data may be biased towards\n",
    "# some particular classes in the beginning - depending on the method used to obtain examples\n",
    "np.random.shuffle(data) \n",
    "\n",
    "X = data[:,1:17]\n",
    "y = data[:,0]\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "\n",
    "# Convert to range 0 - 25 ('A' -> 0, 'Z' -> 25)\n",
    "ord_ = np.vectorize(ord)\n",
    "y = ord_(y) - 65 \n",
    "\n",
    "total_size = X.shape[0]\n",
    "training_set_size = int(total_size * (0.8))\n",
    "test_set_size = total_size - training_set_size\n",
    "\n",
    "Xtrain = X[0:training_set_size, :]\n",
    "XtrainNorm = preprocessing.scale(Xtrain.astype(np.float64)) # normalize to mean = 0 and unit variance\n",
    "XtrainNorm = np.append(np.ones((XtrainNorm.shape[0],1)), XtrainNorm, axis = 1)\n",
    "ytrain = y[0:training_set_size]\n",
    "\n",
    "Xtest = X[(training_set_size):(training_set_size + test_set_size), :]\n",
    "XtestNorm = preprocessing.scale(Xtest.astype(np.float64)) # normalize to mean = 0 and unit variance\n",
    "XtestNorm = np.append(np.ones((XtestNorm.shape[0],1)), XtestNorm, axis = 1)\n",
    "ytest = y[(training_set_size): (training_set_size + test_set_size)]\n",
    "\n",
    "num_labels = 26\n",
    "m,n = XtrainNorm.shape\n",
    "maxiter = 100\n",
    "Theta = np.zeros((n, num_labels))\n",
    "lmbda = 0.3\n",
    "\n",
    "partitions = 10\n",
    "cost_history = np.zeros((num_labels, partitions))\n",
    "\n",
    "print(\"Training : [\", end=\"\")\n",
    "for k in range(num_labels):\n",
    "    print(chr(k + 65) + \"...\", end=\"\")\n",
    "    theta_init_k = np.zeros((n, 1))\n",
    "    # theta_k = gradientDescentLogisticRegression(theta_init_k, XtrainNorm, (ytrain == k) , maxiter, lmbda)\n",
    "    cost_history[k], theta_k = gradientDescentWrapper(theta_init_k, XtrainNorm, (ytrain == k) , maxiter, lmbda, partitions)\n",
    "    Theta[:,k] = np.copy(theta_k)\n",
    "print(\"]\", end=\"\")\n",
    "\n",
    "plt.plot(np.sum(cost_history, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating accuracy\n",
    "\n",
    "Once we have the value for $\\Theta$, we can predict each training and test example, and cross verify with ground truth to find out the accuracy of the model.\n",
    "\n",
    "Following function uses $\\Theta$ to calculate probabilities of each example for each labal class, and then predicts class having highest probability as the hypothesis for that example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predictOneVsAll(Theta, X):\n",
    "    # Theta -> n x K\n",
    "    # X -> m x n\n",
    "    # H -> m x K\n",
    "    sig = np.vectorize(sigmoid)\n",
    "    H = sig(np.dot(X, Theta))\n",
    "    pred = np.argmax(H, axis = 1)\n",
    "    pred = pred.reshape(pred.shape[0], 1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate accuracies, we will apply the above function to training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for 16000 samples (percentage) [[ 72.24375]]\n",
      "Test accuracy for 4000 samples (percentage) [[ 72.25]]\n"
     ]
    }
   ],
   "source": [
    "pred_train = predictOneVsAll(Theta, XtrainNorm)\n",
    "pred_test = predictOneVsAll(Theta, XtestNorm)\n",
    "\n",
    "def accuracy(pred, y):\n",
    "    m = y.shape[0]\n",
    "    acc = sum(pred == y) / m\n",
    "    return acc\n",
    "\n",
    "acc_train = accuracy(pred_train, ytrain)\n",
    "acc_test = accuracy(pred_test, ytest)\n",
    "print(\"Training accuracy for \" + str(training_set_size) + \" samples (percentage)\", acc_train * 100)\n",
    "print(\"Test accuracy for \" + str(test_set_size) + \" samples (percentage)\", acc_test * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
