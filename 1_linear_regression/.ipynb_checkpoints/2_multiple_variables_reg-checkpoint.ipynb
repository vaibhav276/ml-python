{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression: Multiple variables with Regularization\n",
    "\n",
    "This notebook is an extension to <a href=\"1_multiple_variables.ipynb\"> Linear Regresssion: Multiple Variables </a> notebook. Please refer to it before reading this. We will enhance the solution by performing regularization while calculating the values of $\\theta$.\n",
    "\n",
    "## What is regularization?\n",
    "Regularization is a technique to improve the generalization of a model. For a given set of training examples, there are an infinite number of curves that can fit, most of which are overcomplicated - having too many curves and angles. These kind of overcompilcated models don't usually generalize well to new examples outside the training set. This problem is called \"overfitting\". It is also called \"High variance problem\".\n",
    "\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/f/fd/75hwQ.jpg\" />\n",
    "\n",
    "On the other extreme, we have something called as \"underfitting\" or \"High bias problem\", in which the curve in question is too rigid.\n",
    "\n",
    "In the above figure, the green and blue functions both incur zero loss on the given data points. A learned model can be induced to prefer the green function, which may generalize better to more points drawn from the underlying unknown distribution. With regularization we attempt to solve the problem of overfitting by introducing a regularization term to the cost function, which essentially penalizes high values of $\\theta$, keeping the variance of the model managable.\n",
    "\n",
    "## The new cost function\n",
    "\n",
    "The new cost function look like this:\n",
    "\n",
    "$$\n",
    "J = \\frac {1}{2m} \\sum_{i=1}^{m} [h_\\theta(x^{(i)}) - y^{(i)}]^2 + \\frac \\lambda {2m} \\sum_{j = 1}^n \\theta^2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a variable to control the amount of regularization.\n",
    "\n",
    "Note that if we introduce an intercept term $x_0$ and its corresponding $\\theta_0$, we don't include $\\theta_0$ in the summation of $\\theta$ because $x_0$ is a constant term.\n",
    "\n",
    "The following code implements the new cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hypothesis function\n",
    "def hypothesisLinearRegression(theta, X):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "# Function to calculate cost for a given set of parameters (theta)\n",
    "def costLinearRegressionReg(theta, X, y, lmbda):\n",
    "    m = X.shape[0]\n",
    "    diff = hypothesisLinearRegression(theta, X) - y\n",
    "    cost = (1.0/(2 * m)) * (np.dot(diff.T, diff))\n",
    "    theta_ = np.copy(theta)\n",
    "    theta_[0][0] = 0;\n",
    "    reg = (lmbda/(2 * m)) * np.dot(theta_.T, theta_)\n",
    "    cost = cost + reg;\n",
    "    return cost[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Gradient Descent\n",
    "\n",
    "Since we have a new cost function, the gradient function will also be different. The new gradient descent algorithm is:\n",
    "\n",
    "repeat till convergence: {\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_0 := \\theta_0 - \\alpha \\left(\\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_0^{(i)}\\right) \\\\\n",
    "\\theta_1 := \\theta_0 - \\alpha \\left(\\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_1^{(i)} + \\frac \\lambda m \\theta_1\\right) \\\\\n",
    "\\theta_2 := \\theta_0 - \\alpha \\left(\\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_2^{(i)} + \\frac \\lambda m \\theta_2\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n := \\theta_0 - \\alpha \\left(\\frac 1m \\sum_{1=1}^m ( h_\\theta(x^{(i)}) - y^{(i)}).x_n^{(i)} + \\frac \\lambda m \\theta_n\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "}\n",
    "\n",
    "The following is our new gradient descent algorithm implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescentReg(theta, X, y, alpha, num_iters, lmbda):\n",
    "    m = X.shape[0]\n",
    "    cost_history = [costLinearRegressionReg(theta, X, y, lmbda)]\n",
    "    \n",
    "    theta_ = np.copy(theta)\n",
    "    theta_[0][0] = 0\n",
    "\n",
    "    for i in range(1,num_iters):\n",
    "        H = hypothesisLinearRegression(theta, X)\n",
    "        D = H - y\n",
    "        theta = theta - ( (alpha/m) * ( np.dot(D.T, X).T + (lmbda * theta_) ) )\n",
    "        cost_history = np.append(cost_history, costLinearRegressionReg(theta, X, y, lmbda))\n",
    "    \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Normal equation\n",
    "\n",
    "Similar to gradient descent, we can also regularize our normal equation. The new equation is:\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX + \\lambda L)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "L = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "& 1 \\\\\n",
    "& & 1 \\\\\n",
    "& & & \\ddots \\\\\n",
    "& & & & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$L$ is a matrix with 0 at the top and 1's for the rest of the diagonal and 0's everywhere else.\n",
    "\n",
    "The following code implements our new normal equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalEquationReg(X, y, lmbda):\n",
    "    from numpy.linalg import inv\n",
    "    L = np.identity(X.shape[1])\n",
    "    L[0][0] = 0\n",
    "    \n",
    "    theta= np.dot( np.dot( inv( np.dot(X.T, X) + (lmbda * L) ), X.T ), y )\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying new algorithms on data\n",
    "\n",
    "Following code applies our new regularized algorithms to the same data set that we used in <a href=\"1_multiple_variables.ipynb\"> Linear Regresssion: Multiple Variables </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for gradient descent method:\n",
      "Training error(160 examples) = [[ 1.27875824]]\n",
      "Test error(40 examples) = [[ 2.6757446]]\n",
      "Theta [[ 13.82270155]\n",
      " [  3.96538189]\n",
      " [  2.75925744]\n",
      " [ -0.15907728]]\n",
      "\n",
      "Output for normal equation method:\n",
      "Training error(160 examples) = [[ 1.27795181]]\n",
      "Test error(40 examples) = [[ 2.65217116]]\n",
      "Theta [[ 13.855     ]\n",
      " [  3.94599117]\n",
      " [  2.7630588 ]\n",
      " [ -0.17390221]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73f5154b70>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGERJREFUeJzt3XuQVOWd//H3FxAQURAvowIJeCGAK0gMBm+xMZqQjVGz\nWxHNbsKqSbbKmBiztRH2txXnn7XUVPanbhJr4yWLyepvze5q4ia1CxomJimjoCgIIsKKAaLjJeAV\nAsrz++P0yDjODExPT5/uPu9X1Sm6T58+/eVU16efeZ5znhMpJSRJzW1Q3gVIkgaeYS9JBWDYS1IB\nGPaSVACGvSQVgGEvSQXQa9hHxG0R0R4RKzut+1ZEPBkRj0fEf0bEqE6vLYiIpyNiTUR8bCALlyTt\nvT217H8AzOmybhFwbEppOrAWWAAQEVOBucDU8nu+FxH+5SBJdaDXME4p/QrY0mXd4pTSrvLTh4Bx\n5cfnAnemlHamlDYA64ATq1uuJKkS/W15Xwz8vPz4CGBTp9c2AWP7uX9JUhVUHPYR8X+AHSmlO3rZ\nzLkYJKkODKnkTRHxV8CfAh/ttHozML7T83HldV3f6w+AJFUgpRSVvrfPLfuImAP8LXBuSml7p5d+\nClwQEUMjYiJwDPBwd/tIKblUabnqqqtyr6GZFo+nx7Nel/7qtWUfEXcCpwMHR8RG4Cqys2+GAosj\nAuDBlNKlKaXVEXEXsBp4C7g0VaNCSVK/9Rr2KaULu1l9Wy/bXw1c3d+iJEnV5XnwDa5UKuVdQlPx\neFaXx7N+RK17WiLC3h1J6qOIINVygFaS1HgMe0kqAMNekgrAsJekAjDsJakADHtJKgDDXpIKwLCX\npAIw7CWpAAx7SSoAw16SCsCwl6QCyCXsd+zI41MlqbhyCfsNG/L4VEkqrlzCfv36PD5VkorLsJek\nAsgl7Nety+NTJam4bNlLUgEY9pJUALncg3b48MQbb8Agz/KXpL3SkPegHT0afv/7PD5Zkoopl7A/\n6ii7ciSplnIJ+6OP9owcSaolW/aSVAC27CWpAHIJ+2OOgaefzuOTJamYeg37iLgtItojYmWndWMi\nYnFErI2IRRExutNrCyLi6YhYExEf62m/HWFf47M+Jamw9tSy/wEwp8u6+cDilNIk4P7ycyJiKjAX\nmFp+z/ciotv9jxoF++0Hzz3Xn9IlSXur17BPKf0K2NJl9TnAwvLjhcB55cfnAnemlHamlDYA64AT\ne9r3pEmwdm0lJUuS+qqSPvuWlFJ7+XE70FJ+fASwqdN2m4CxPe3EfntJqp1+DdCmbK6F3nree3zN\nlr0k1c6QCt7THhGHpZSej4jDgRfK6zcD4zttN6687j1aW1tZvRpWrIBPfrJEqVSqoAxJal5tbW20\ntbVVbX97nAgtIiYA96aUjis/vw54OaV0bUTMB0anlOaXB2jvIOunHwvcBxydunxARKSUEitXwty5\nsHp11f4vktS0+jsRWq9hHxF3AqcDB5P1z38T+AlwF/A+YANwfkppa3n7vwMuBt4CLk8p/U83+0wp\nJbZtgzFj4PXXYfDgSsuXpGIY0LAfCB1hD/C+98EvfwkTJ9a0BElqOA05xXGHSZPgqafyrECSiiHX\nsJ88GdasybMCSSqGXMN+yhR48sk8K5CkYjDsJakADHtJKoBcw/6ww2DnTnjxxTyrkKTml2vYR9i6\nl6RayDXswbCXpFrIPeynTjXsJWmg5R72tuwlaeAZ9pJUALnOjQPw9tuw//7Q3p79K0l6r4aeGwey\nGS8nTXLaBEkaSLmHPdiVI0kDrS7C3jNyJGlg1UXY27KXpIFVN2Hv7QklaeDkfjYOwI4dcMAB8Mor\nMGxYTcuRpIbQ8GfjAAwdCu9/Pzz9dN6VSFJzqouwB/vtJWkgGfaSVAB1E/ZTp8KqVXlXIUnNqW7C\nfto0WLky7yokqTnVxdk4AH/8I4weDX/4A+y7b01LkqS61xRn40B2yuUxx9iVI0kDoW7CHmD6dFix\nIu8qJKn51FXYT5sGjz+edxWS1HzqKuxt2UvSwKibAVrIbmAyZQq8/DJExcMQktR8chugjYgrIuKJ\niFgZEXdExLCIGBMRiyNibUQsiojRfdlnSwvssw9s2lRpVZKk7lQU9hExFvgKcEJK6ThgMHABMB9Y\nnFKaBNxfft4nduVIUvX1p89+CDAiIoYAI4DfA+cAC8uvLwTO6+tOp093kFaSqq2isE8pbQa+DfyO\nLOS3ppQWAy0ppfbyZu1AS1/3PW2aLXtJqrYhlbwpIg4ka8VPAF4BfhwRf9l5m5RSiohuR2JbW1vf\neVwqlSiVSu88nz4drr66kqokqXm0tbXR1tZWtf1VdDZORHwG+HhK6Qvl558DZgFnALNTSs9HxOHA\nkpTS5C7v7fFsHMhuZDJqlNMmSFJneZ2N8ywwKyL2jYgAzgRWA/cC88rbzAPu6euOhw6FSZOcNkGS\nqqnSPvuHgX8HHgU6eti/D1wDnBURa8la+ddUsn8HaSWpuurqoqoO3/42/O53cMMNNSpKkupc08x6\n2Zlz5EhSddVly/6FF+ADH8gGaZ02QZKatGV/6KGw337wzDN5VyJJzaEuwx5g5kxYujTvKiSpOdR1\n2C9blncVktQc6jbsP/QhW/aSVC11OUAL2eDshAmwZQsMHjzwdUlSPWvKAVqAMWOygdqnnsq7Eklq\nfHUb9uAgrSRVi2EvSQVg2EtSAdTtAC3A669n96XdsiWbDVOSiqppB2gBRo6EiRPhiSfyrkSSGltd\nhz3YlSNJ1WDYS1IBGPaSVAB1PUAL8Mc/woEHwksvwYgRA1iYJNWxph6gBRg2DKZOheXL865EkhpX\n3Yc9wIc/DL/9bd5VSFLjaoiwP/VU+M1v8q5CkhpX3ffZA2zcCCecAO3t3qZQUjE1fZ89wPjxsO++\n8PTTeVciSY2pIcIe4JRT4Ne/zrsKSWpMDRP2p55q2EtSpRoq7B2klaTKNEzYH3tsNkD7wgt5VyJJ\njadhwn7wYDj5ZFv3klSJhgl7cJBWkipVcdhHxOiI+PeIeDIiVkfEhyNiTEQsjoi1EbEoIkZXs1gH\naSWpMv1p2d8A/DylNAWYBqwB5gOLU0qTgPvLz6tm5szsRiZvvlnNvUpS86so7CNiFHBaSuk2gJTS\nWymlV4BzgIXlzRYC51WlyrIRI2DaNHj44WruVZKaX6Ut+4nAixHxg4h4NCJujoj9gJaUUnt5m3ag\npSpVdmK/vST13ZB+vO+DwGUppaURcT1dumxSSikiup0Ep7W19Z3HpVKJUqm01x982mnw3e9WULEk\nNZC2tjba2tqqtr+KJkKLiMOAB1NKE8vPTwUWAEcCs1NKz0fE4cCSlNLkLu/t80RonW3dms2V8+KL\nMHx4xbuRpIaSy0RoKaXngY0RMam86kxgFXAvMK+8bh5wT6WF9WT06OxmJg8+WO09S1LzqrQbB+Ar\nwL9GxFBgPXARMBi4KyIuATYA5/e7wm6ceSbcfz/Mnj0Qe5ek5tMQ89l3tWQJLFjg3askFUd/u3Ea\nMuy3b4dDDsluajK6qpdtSVJ9KsTNS7oaPhxOOgmqOFAtSU2tIcMedvfbS5L2rKHD/r778q5CkhpD\nw4b98cdn59pv2pR3JZJU/xo27AcNgjPOsCtHkvZGw4Y9wEc/athL0t5oyFMvO6xfn82Vs3kzRMUn\nJElS/SvkqZcdjjwym/Z4xYq8K5Gk+tbQYR8BZ58N//VfeVciSfWtocMesrC/9968q5Ck+tbQffYA\nO3ZASwusWZP9K0nNqNB99gBDh8JZZ8HPfpZ3JZJUvxo+7MF+e0nak4bvxgF46SU46ihob/fuVZKa\nU+G7cQAOPhiOO85ZMCWpJ00R9mBXjiT1pmnC/lOfyk7BrHGvlCQ1hKYJ+6lTYfBgWLky70okqf40\nTdhHwDnnwN13512JJNWfpgl7gPPPh7vuyrsKSao/TRX2s2bBa6/BE0/kXYkk1ZemCvtBg+Azn4F/\n+7e8K5Gk+tJUYQ8wd27WleNZOZK0W9OF/cyZsHMnPP543pVIUv1ourCPyAZq7cqRpN2aLuxhd9jb\nlSNJmaYM+xkzsgusHnkk70okqT70K+wjYnBELI+Ie8vPx0TE4ohYGxGLImJ0dcrsa13ZQK1dOZKU\n6W/L/nJgNdDRYTIfWJxSmgTcX36eiwsvhDvugLfeyqsCSaofFYd9RIwD/hS4BeiYY/kcYGH58ULg\nvH5V1w/HHgvjxsGiRXlVIEn1oz8t+/8L/C2wq9O6lpRSe/lxO5DrXWEvuQRuuy3PCiSpPgyp5E0R\ncTbwQkppeUSUutsmpZQiotvzYVpbW995XCqVKJW63UW/zZ0L3/gGvPgiHHLIgHyEJA2ItrY22qp4\nR6aKbksYEVcDnwPeAoYDBwD/CcwESiml5yPicGBJSmlyl/dW/baEvfn857Ozc664omYfKUlVl8tt\nCVNKf5dSGp9SmghcAPwipfQ54KfAvPJm84B7Ki2sWi6+GG691XPuJRVbtc6z74jSa4CzImItcEb5\nea4+8hHYtg2WLcu7EknKT0XdOP36wBp34wD8wz/Apk1w0001/VhJqpr+duMUIuw3bYJp02DjRthv\nv5p+tCRVRS599o1m3Dg47TT40Y/yrkSS8lGIsAe4/HK48UYHaiUVU2HCfvbs7E5W992XdyWSVHuF\nCfsI+NrX4IYb8q5EkmqvEAO0HbZtg/e/H379a5g0KZcSJKkiDtD2wb77whe/CP/0T3lXIkm1VaiW\nPcDmzXDccfC//wujc5ltX5L6zpZ9H40dC3PmwM03512JJNVO4Vr2ACtWwMc/DuvXw4gRuZYiSXvF\nln0Fpk2Dk06ydS+pOArZsgdYvhzOPjtr3Q8fnnc1ktQ7W/YVmjEDTjghm/5YkppdYVv2AEuXwp/9\nGaxbB8OG5V2NJPXMln0/zJwJf/In8C//knclkjSwCt2yB/jtb+H88+Gpp7KLriSpHtmy76dZs+DE\nE+H66/OuRJIGTuFb9pD12c+aBatXw6GH5l2NJL2Xd6qqkq99DXbuhO9+N+9KJOm9DPsqefllmDwZ\nHngApkzJuxpJejf77KvkoINg/ny48sq8K5Gk6jPsO7nsMli1Cv77v/OuRJKqy7DvZNgw+N734NJL\n4Y038q5GkqrHPvtufPazMG4cXHdd3pVIUsYB2gHQ3p7d4GTRIjj++LyrkSQHaAdESwtcfTV86Uvw\n9tt5VyNJ/WfY9+Dii7PpE268Me9KJKn/7Mbpxfr12ZW1v/hF1q0jSXnJpRsnIsZHxJKIWBURT0TE\nV8vrx0TE4ohYGxGLIqKhb+l91FHwrW9lA7bbt+ddjSRVrqKWfUQcBhyWUnosIkYCjwDnARcBL6WU\nrouIK4EDU0rzu7y3YVr2ACnB3LlwxBFOliYpP3VxNk5E3AN8p7ycnlJqL/8gtKWUJnfZtqHCHmDL\nFpg+Hb7/fZgzJ+9qJBVR7mfjRMQEYAbwENCSUmovv9QOtPR3//XgwAPh9tuzQduNG/OuRpL6rl9h\nX+7C+Q/g8pTSa51fKzffG6sJ34tSCb7+dfj0p2HbtryrkaS+GVLpGyNiH7Kg/2FK6Z7y6vaIOCyl\n9HxEHA680N17W1tb33lcKpUolUqVllFTf/M38Oij2fn3t98OUfEfVJLUu7a2Ntra2qq2v0oHaANY\nCLycUrqi0/rryuuujYj5wOhGH6Dt6s034ZRT4POfhyuu2PP2klQNuQzQRsSpwAPACnZ31SwAHgbu\nAt4HbADOTylt7fLehg57gGefzc6/v/lmOPvsvKuRVAR1cTZOnz6wCcIeshuVf+pT8JOfwMkn512N\npGaX+9k4RTVrFvzwh9mA7apVeVcjSb0z7Pthzhz4x3/M/n322byrkaSeVXw2jjJ/8RfZ/Wtnz87m\n0JkwIe+KJOm9DPsq+OpXYdAgOP10uP9+OProvCuSpHcz7Kvksstg6NDs4qv77oPJk/f4FkmqGcO+\nir70pew+trNnw913Z4O4klQPHKCtsnnz4JZbstMyf/zjvKuRpIzn2Q+Qxx7LAv/LX4Yrr3RqBUn9\n40VVdWzz5izwP/CBbHrk/ffPuyJJjcqLqurY2LHwm99kIf+hD8HKlXlXJKmoDPsBtu++Wav+7/8e\nzjgD/vmfs7tfSVIt2Y1TQ6tXZ7NlHnxwNona+PF5VySpUdiN00CmToUHH4RTT4UPfhBuvRV27cq7\nKklFYMs+JytWwCWXZBdifec7MGNG3hVJqme27BvUtGnw0ENw0UXwiU/ApZfCSy/lXZWkZmXY52jQ\nIPjCF+DJJ2HIkGyKhdZWePXVvCuT1GwM+zpw4IFw442wdCk880w2kdq118Irr+RdmaRmYdjXkYkT\nYeFCWLIkOyf/yCOzq2+fey7vyiQ1OsO+Dh17LPzoR/DII7BtW3YWz4UXwgMPeI6+pMp4Nk4D2LIF\nbr8dbrop69u/6CK44ILsCl1JxeDcOAWSEvzyl9m9b+++G44/Hj77WfjzP8/6/SU1L8O+oLZvh5//\nHO64AxYvhpNPhk9+MlsmTsy7OknVZtiLV1+FRYvgZz/LfgAOOigL/bPOyn4ERo7Mu0JJ/WXY6112\n7YJly7LgX7IEHn00G/D9yEeye+TOmpXNzSOpsRj26tX27dmVug88kPX3L10KY8ZkUy53LNOn+wMg\n1TvDXn2yaxesW5eF/rJl2bJyZTZHz7HHZqd5dixTpkBLi3fZkuqBYa9+Sym7cGv16vcu27fDhAnZ\noO/EibsfT5gAhx8OhxwCgwfn/B+QCsCw14B69dVsCoeuy7PPZj8QW7fCoYdmwd91OfjgbLB4zJhs\nOeig7GYu/qUg9V3dhX1EzAGuBwYDt6SUru3yumHfRHbsgPb2LPi7Li+/nC1/+MPux7D7B2D06OyW\njQcckP3bsXR9PnIkDB+e/VB0LJ2f+5eFiqCuwj4iBgNPAWcCm4GlwIUppSc7bWPYV1FbWxulUinv\nMvbam2/uDv+tW+G113Yvr7767ucd6954I5s2Yvv27N+OpeP5kCHd/xjss8+7l6FD37uu6/Lcc20c\ndVTpneeDBmU/JtX4t+u6iN1/5XQ87rx0t74v21ZjH3ujt+0efLCNk04q7XG7vd1fUbc76CAYPLh/\nYT+k0jf24ERgXUppA0BE/D/gXODJ3t6kyjVa2I8YkS3jxlVnfyllf11090Owc+fuZceOdz/vaXns\nsTYmTy6xc2f2w7RrV7a8/XZ1/u14/Pbbu+c5Sum9S3fr+7JtNfaxt8e/N6+80saoUaWq7a+o2z31\n1N5t15tqh/1YYGOn55uAD1f5M6R3RMCwYdkyalT/9/f66/DNb/Z/P8q0tmaL8lftWS/tn5GkOlTt\nPvtZQGtKaU75+QJgV+dB2ojwB0GSKlBPA7RDyAZoPwr8HniYLgO0kqTaq2qffUrprYi4DPgfslMv\nbzXoJSl/Nb+oSpJUezW9LWFEzImINRHxdERcWcvPbhYRsSEiVkTE8oh4uLxuTEQsjoi1EbEoIkbn\nXWc9iojbIqI9IlZ2WtfjsYuIBeXv6pqI+Fg+VdevHo5na0RsKn8/l0fEJzq95vHsQUSMj4glEbEq\nIp6IiK+W11fv+5lSqslC1q2zDpgA7AM8Bkyp1ec3ywI8A4zpsu464Bvlx1cC1+RdZz0uwGnADGDl\nno4dMLX8Hd2n/J1dBwzK+/9QT0sPx/Mq4OvdbOvx7P1YHgYcX348kmzsc0o1v5+1bNm/c8FVSmkn\n0HHBlfqu64j8OcDC8uOFwHm1LacxpJR+BWzpsrqnY3cucGdKaWfKLhJcR/YdVlkPxxPe+/0Ej2ev\nUkrPp5QeKz9+nexC1LFU8ftZy7Dv7oIrb5nddwm4LyKWRcQXy+taUkrt5cftQEs+pTWkno7dEWTf\n0Q5+X/feVyLi8Yi4tVO3g8dzL0XEBLK/mB6iit/PWoa9I8HVcUpKaQbwCeDLEXFa5xdT9jeex7oC\ne3HsPK57dhMwETgeeA74di/bejy7iIiRwH8Al6eUXuv8Wn+/n7UM+83A+E7Px/PuXybthZTSc+V/\nXwTuJvvTrT0iDgOIiMOBF/KrsOH0dOy6fl/HldepFymlF1IZcAu7uxY8nnsQEfuQBf0PU0r3lFdX\n7ftZy7BfBhwTERMiYigwF/hpDT+/4UXEiIjYv/x4P+BjwEqy4zivvNk84J7u96Bu9HTsfgpcEBFD\nI2IicAzZRYLqRTmQOnya7PsJHs9eRUQAtwKrU0rXd3qpat/Pak+E1qPkBVfV0ALcnX0vGAL8a0pp\nUUQsA+6KiEuADcD5+ZVYvyLiTuB04OCI2Ah8E7iGbo5dSml1RNwFrAbeAi4tt1ZV1s3xvAooRcTx\nZF0KzwB/DR7PvXAK8JfAiohYXl63gCp+P72oSpIKoKYXVUmS8mHYS1IBGPaSVACGvSQVgGEvSQVg\n2EtSARj2klQAhr0kFcD/B0Q67Wu+YbdzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73f52790f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
    "\n",
    "# Extract into separate matrices for input and output\n",
    "data = np.matrix(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,0:3]\n",
    "y = data[:,3]\n",
    "\n",
    "y = np.reshape(y, (y.shape[0], 1))\n",
    "\n",
    "# Dividing data into two sets\n",
    "# 1. Training set\n",
    "# 2. Test set\n",
    "# A good set of parameters should not only prove good for training set,\n",
    "# but also for test set\n",
    "total_size = X.shape[0]\n",
    "training_set_size = int(total_size * (0.8))\n",
    "test_set_size = total_size - training_set_size\n",
    "\n",
    "Xtrain = X[0:training_set_size, :]\n",
    "XtrainNorm = preprocessing.scale(Xtrain.astype(np.float64)) # normalize to mean = 0 and unit variance\n",
    "XtrainNorm = np.append(np.ones((XtrainNorm.shape[0],1)), XtrainNorm, axis = 1)\n",
    "ytrain = y[0:training_set_size]\n",
    "\n",
    "Xtest = X[(training_set_size):(training_set_size + test_set_size), :]\n",
    "XtestNorm = preprocessing.scale(Xtest.astype(np.float64)) # normalize to mean = 0 and unit variance\n",
    "XtestNorm = np.append(np.ones((XtestNorm.shape[0],1)), XtestNorm, axis = 1)\n",
    "ytest = y[(training_set_size): (training_set_size + test_set_size)]\n",
    "\n",
    "alpha = 0.03\n",
    "num_iters = 200\n",
    "lmbda = 1\n",
    "\n",
    "theta_gd = np.zeros((X.shape[1] + 1, 1))\n",
    "theta_gd, cost_history = gradientDescentReg(theta_gd, XtrainNorm, ytrain, alpha, num_iters, lmbda)\n",
    "\n",
    "print(\"Output for gradient descent method:\")\n",
    "print (\"Training error(\" + str(training_set_size) + \" examples) = \" + str(costLinearRegressionReg(theta_gd, XtrainNorm, ytrain, lmbda)))\n",
    "print (\"Test error(\" + str(test_set_size) + \" examples) = \" + str(costLinearRegressionReg(theta_gd, XtestNorm, ytest, lmbda)))\n",
    "print (\"Theta\", theta_gd)\n",
    "\n",
    "theta_ne = normalEquationReg(XtrainNorm, ytrain, lmbda)\n",
    "\n",
    "print(\"\\nOutput for normal equation method:\")\n",
    "print (\"Training error(\" + str(training_set_size) + \" examples) = \" + str(costLinearRegressionReg(theta_ne, XtrainNorm, ytrain, lmbda)))\n",
    "print (\"Test error(\" + str(test_set_size) + \" examples) = \" + str(costLinearRegressionReg(theta_ne, XtestNorm, ytest, lmbda)))\n",
    "print (\"Theta\", theta_ne)\n",
    "\n",
    "plt.plot(cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected to get lower values of $\\theta$ components (except $\\theta_0$) when using regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
