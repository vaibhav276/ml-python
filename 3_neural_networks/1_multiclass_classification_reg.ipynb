{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Multiclass classification with Regularization\n",
    "\n",
    "This notebook covers the core concepts of neural networks. It reuses some explanation in another notebook: <a href=\"../2_logistic_regression/2_multiclass_classification_reg.ipynb\"> Logistic Regression: Multiclass Classification with Regularization </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We are going to use the same data set that was used for <a href=\"../2_logistic_regression/2_multiclass_classification_reg.ipynb\"> Logistic Regression: Multiclass Classification with Regularization </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0  T   2   8   3   5   1   8  13   0   6   6  10   8   0   8   0   8\n",
       "1  I   5  12   3   7   2  10   5   5   4  13   3   9   2   8   4  10\n",
       "2  D   4  11   6   8   6  10   6   2   6  10   3   7   3   7   3   9\n",
       "3  N   7  11   6   6   3   5   9   4   6   4   4  10   6  10   2   8\n",
       "4  G   2   1   3   1   1   8   6   6   6   6   5   9   1   7   5  10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as op\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "data = pd.read_csv('../2_logistic_regression/letter-recognition.data', header=None)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Neural Networks work?\n",
    "\n",
    "Where logistic regression applies a sigmoid function on the input features to produce a hypothesis directly, neural networks have one or more layers (called \"hidden layers\") on which the same logistic function is applied again and again, once at each layer, until the final layer, which is the output hypothesis. So essentially, the input features pass through multiple applications of logistic regression to finally produce a hypothesis.\n",
    "\n",
    "<img src = \"http://docs.opencv.org/2.4/_images/mlp.png\" />\n",
    "\n",
    "**Motivation**: The rationale behind having hidden layers vs. logistic regression is that when we use multiple layers, we can easily/efficiently represent a pretty complicated hypothesis function spread across a sequence of layers. When compared to simple logistic regression, a similar function would require us to create orders of magnitudes of new artificial polynomial features. The order of such artificial features easily goes to a several thousands even for low complexity functions. On the other hand, using only a few layers in a neural network, we can easily represent a very complex hypothesis function on the original set of features.\n",
    "\n",
    "For a simple neural network as follows:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_0 \\newline\n",
    "x_1 \\newline\n",
    "x_2 \\newline\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "a_1^{(2)} \\newline\n",
    "a_2^{(2)} \\newline\n",
    "a_3^{(2)} \\newline\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "h_\\theta(x)\n",
    "$$\n",
    "\n",
    "The values for each of the \"activation\" nodes is obtained as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) \\newline\n",
    "a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) \\newline\n",
    "a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) \\newline\n",
    "h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) \\newline\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where \n",
    "$\\Theta^{(1)}$ is the $\\Theta$ corresponding to layer 1, which means the \"weights\" that should be applied to units in layer 1 in order to get layer 2 units.\n",
    "\n",
    "and $g$ is the sigmoid function\n",
    "\n",
    "So, in case of neural networks, we will have a a set of $\\Theta$'s, where each $\\Theta$ corresponds to one layer. Also, just like logistic regression, we will add intercept term (with value 1) at each layer.\n",
    "\n",
    "The following code implements the hypothesis function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Hypothesis for a single layer\n",
    "def hypothesisLayer(Theta, A):\n",
    "    sig = np.vectorize(sigmoid)\n",
    "    return sig(np.dot(A, Theta))\n",
    "\n",
    "# Hypothesis of the network\n",
    "def hypothesisNeuralNetwork(Theta_list, X):\n",
    "    num_layers = len(Theta_list)\n",
    "    A_l = np.copy(X)\n",
    "    for l in range(num_layers):\n",
    "        Theta = Theta_list[l]\n",
    "        A_l_1 = hypothesisLayer(Theta, A_l)\n",
    "        # Add intercept term before feeding to next Theta\n",
    "        A_l = np.append(np.ones((A_l_1.shape[0], 1)), A_l_1, axis = 1)\n",
    "    \n",
    "    return A_l[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of implementing XNOR function in neural network. Here we have two layers: First layer calculates AND operation ($\\theta = $[ -30, 20, 20 ]) and NOR operation ($\\theta = $[10, -20, -20]), and the second layer calculates the OR operation ($\\theta = $[-10, 20 20]). Hence when we combine them we get XNOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [[1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]\n",
      "Output (XNOR) [[  9.99954561e-01]\n",
      " [  4.54803785e-05]\n",
      " [  4.54803785e-05]\n",
      " [  9.99954561e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = np.matrix('1 0 0; 1 0 1; 1 1 0; 1 1 1')\n",
    "Theta_1 = np.matrix('-30 10; 20 -20; 20 -20')\n",
    "Theta_2 = np.matrix('-10; 20; 20')\n",
    "\n",
    "print(\"Input\", X)\n",
    "\n",
    "Theta_list = [Theta_1, Theta_2]\n",
    "\n",
    "print(\"Output (XNOR)\", hypothesisNeuralNetwork(Theta_list, X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, hypothesis is approximately 1 for values where either both inputs are 0 or both are 1. It is approximately 0 for remaining inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
